import pandas as pd
import numpy as np

import os
from sklearn.preprocessing import StandardScaler

BASE_DIR = r"C:\Users\Islam\Documents\projet_ML"
REPORTS_DIR = os.path.join(BASE_DIR, "reports")
PROCESSED_DIR = os.path.join(BASE_DIR, "data", "processed")  # D√âPLAC√â ICI

os.makedirs(REPORTS_DIR, exist_ok=True)
os.makedirs(PROCESSED_DIR, exist_ok=True) 

df = pd.read_csv(r"C:\Users\Islam\Documents\projet_ML\data\raw\retail_customers_COMPLETE_CATEGORICAL.csv")

#============================
# √âTAPE 1: PARSING DES DATES

print("\nüìÖ √âtape 1: Parsing des dates...")
print("AVANT:", df['RegistrationDate'].head(3).tolist())
df['RegistrationDate'] = pd.to_datetime(df['RegistrationDate'], dayfirst=True, errors='coerce')
# EXTRACTION year/month/day
df['RegYear'] = df['RegistrationDate'].dt.year
df['RegMonth'] = df['RegistrationDate'].dt.month
df['RegDay'] = df['RegistrationDate'].dt.day

# SUPPRESSION de la colonne originale
df = df.drop(columns=['RegistrationDate'])

print("APR√àS:", df[['RegYear', 'RegMonth', 'RegDay']].head(3).to_string())
print("‚úÖ Dates trait√©es")

#============================
#√âTAPE 2: AGE - 30% MANQUANTS

print("\nüîß √âtape 2: Imputation Age...")

# AVANT: V√©rifier
missing_before = df['Age'].isnull().sum()
print(f"Manquants AVANT: {missing_before} ({missing_before/len(df)*100:.1f}%)")

# M√âTHODE : M√©diane
median_age = df['Age'].median()
df['Age'] = df['Age'].fillna(median_age) 
# APR√àS: V√©rifier
missing_after = df['Age'].isnull().sum()
print(f"Manquants APR√àS: {missing_after}")
print(f"M√©diane utilis√©e: {median_age:.1f} ans")

print("‚úÖ Age trait√©")

#============================
# √âTAPE 3: VALEURS ABERRANTES 

print("\nüîß √âtape 3: Correction valeurs aberrantes...")

#  SupportTicketsCount ‚Üí M√©diane
if 'SupportTicketsCount' in df.columns:
    # Compter les aberrants
    aberrant = df['SupportTicketsCount'].isin([999, -1]).sum()
    print(f"SupportTicketsCount: {aberrant} valeurs aberrantes (-1, 999)")
    
    # Remplacer par NaN puis m√©diane
    df['SupportTicketsCount'] = df['SupportTicketsCount'].replace([999, -1], np.nan)
    median_val = df['SupportTicketsCount'].median()
    df['SupportTicketsCount'] = df['SupportTicketsCount'].fillna(median_val)
    print(f"  ‚Üí Remplac√©s par m√©diane: {median_val}")

# SatisfactionScore ‚Üí M√©diane
if 'SatisfactionScore' in df.columns:
    aberrant = df['SatisfactionScore'].isin([-1, 99]).sum()
    print(f"SatisfactionScore: {aberrant} valeurs aberrantes (-1, 99)")
    
    df['SatisfactionScore'] = df['SatisfactionScore'].replace([-1, 99], np.nan)
    median_val = df['SatisfactionScore'].median()
    df['SatisfactionScore'] = df['SatisfactionScore'].fillna(median_val)
    print(f"  ‚Üí Remplac√©s par m√©diane: {median_val}")

print("‚úÖ √âtape 3 termin√©e")

#======================================
# √âTAPE 4: SUPPRESSION FEATURES INUTILES 

print("\nüóëÔ∏è √âtape 4: Suppression features inutiles...")

# V√©rifier NewsletterSubscribed
if 'NewsletterSubscribed' in df.columns:
    unique_vals = df['NewsletterSubscribed'].unique()
    n_unique = len(unique_vals)
    
    print(f"NewsletterSubscribed: {n_unique} valeur(s) unique(s)")
    print(f"  Valeurs: {unique_vals}")
    
    # Si constante (1 seule valeur) ‚Üí supprimer
    if n_unique == 1:
        df = df.drop(columns=['NewsletterSubscribed'])
        print("  ‚Üí SUPPRIM√âE (constante)")
    else:
        print("  ‚Üí CONSERV√âE (pas constante)")
print("‚úÖ √âtape 4 termin√©e")

# SUPPRESSION CUSTOMERID 
print("\n" + "="*50)
print("AJOUT: SUPPRESSION CUSTOMERID")
print("="*50)

if 'CustomerID' in df.columns:
    print(f"   CustomerID pr√©sent: {df['CustomerID'].nunique()} valeurs uniques")
    df_clean = df.drop(columns=['CustomerID'])
    print(f"   ‚Üí CustomerID SUPPRIM√â")
    print(f"   Dimensions: {df.shape} ‚Üí {df_clean.shape}")
else:
    print("   CustomerID d√©j√† absent")
    df_clean = df.copy()

print("‚úÖ √âtape 4 termin√©e")

#==============================
#√âTAPE 5: EXTRACTION LastLoginIP

print("\nüîß √âtape 5: Extraction LastLoginIP...")

if 'LastLoginIP' in df.columns:
    # AVANT
    print("Exemples IP:", df['LastLoginIP'].head(3).tolist())
    
    # EXTRACTION: premier octet de l'IP
    # "192.168.1.45" ‚Üí "192"
    df['IP_Prefix'] = df['LastLoginIP'].str.split('.').str[0]
    df['IP_Prefix'] = pd.to_numeric(df['IP_Prefix'], errors='coerce')
    
    # SUPPRESSION colonne originale
    df = df.drop(columns=['LastLoginIP'])
    
    # APR√àS
    print(f"IP_Prefix: {df['IP_Prefix'].nunique()} valeurs uniques")
    print("Exemples:", df['IP_Prefix'].head(3).tolist())

print("‚úÖ √âtape 5 termin√©e")

#=================================
# √âTAPE 6: MULTICOLIN√âARIT√â (seuil)

print("\nüîó √âtape 6: Multicolin√©arit√©...")

# V√©rifier corr√©lation MonetaryTotal vs MonetaryAvg
if 'MonetaryTotal' in df.columns and 'MonetaryAvg' in df.columns:
    corr = df['MonetaryTotal'].corr(df['MonetaryAvg'])
    print(f"MonetaryTotal ‚Üî MonetaryAvg: r={corr:.3f}")
    
    if abs(corr) > 0.8:
        df = df.drop(columns=['MonetaryAvg'])
        print("  ‚Üí MonetaryAvg SUPPRIM√âE")
    else:
        print("  ‚Üí Les deux conserv√©es (r < 0.8)")

print("‚úÖ √âtape 6 termin√©e")

#==============================================
# √âTAPE 7: V√âRIFICATION D√âS√âQUILIBRE (detection)

print("\n‚öñÔ∏è √âtape 7: V√©rification d√©s√©quilibre classes...")

# CHURN (0 = fid√®le, 1 = parti)

print("\n--- Churn ---")
churn_counts = df['Churn'].value_counts()
churn_pct = df['Churn'].value_counts(normalize=True) * 100

print(f"   0 (fid√®le):  {churn_counts[0]} clients ({churn_pct[0]:.1f}%)")
print(f"   1 (parti):   {churn_counts[1]} clients ({churn_pct[1]:.1f}%)")

if churn_pct.max() > 80:
    print("   ‚ö†Ô∏è D√©s√©quilibre S√âV√àRE")
else:
    print("   ‚úÖ D√©s√©quilibre MOD√âR√â")

#  ACCOUNTSTATUS 

if 'AccountStatus' in df.columns:
    print("\n--- AccountStatus ---")
    acc_counts = df['AccountStatus'].value_counts()
    acc_pct = df['AccountStatus'].value_counts(normalize=True) * 100
    
    print(acc_counts)
    print(f"\nPourcentages:")
    for status, pct in acc_pct.items():
        print(f"   {status}: {pct:.1f}%")
    
    # V√©rifier quasi-constante (>95%)
    if acc_pct.max() > 95:
        print("   ‚ö†Ô∏è Quasi-constante ‚Üí Suppression possible")
    else:
        print("   ‚úÖ Distribution OK")

print("\n‚úÖ √âtape 7 termin√©e")

#=============================
# √âTAPE 8: FEATURE ENGINEERING 

print("\nüî® √âtape 8: Cr√©ation nouvelles features...")

# MonetaryPerDay = MonetaryTotal / (Recency + 1)
# √âvite division par z√©ro avec +1
if 'MonetaryTotal' in df.columns and 'Recency' in df.columns:
    df['MonetaryPerDay'] = df['MonetaryTotal'] / (df['Recency'] + 1)
    print("   MonetaryPerDay = MonetaryTotal / (Recency + 1)")

# AvgBasketValue = MonetaryTotal / Frequency
if 'MonetaryTotal' in df.columns and 'Frequency' in df.columns:
    df['AvgBasketValue'] = df['MonetaryTotal'] / df['Frequency']
    print("   AvgBasketValue = MonetaryTotal / Frequency")

# TenureRatio = Recency / CustomerTenureDays
if 'Recency' in df.columns and 'CustomerTenureDays' in df.columns:
    df['TenureRatio'] = df['Recency'] / (df['CustomerTenureDays'] + 1)
    print("   TenureRatio = Recency / (CustomerTenureDays + 1)")

print(f"\n‚úÖ √âtape 8 termin√©e - Nouvelles dimensions: {df.shape}")

#========================
# √âTAPE 9: STANDARDSCALER 

print("\nüìè √âtape 9: Standardisation...")


# S√©parer Churn (ne pas le standardiser)
if 'Churn' in df.columns:
    y = df['Churn']
    X = df.drop(columns=['Churn'])
    print("   Churn s√©par√© (pas de standardisation)")
else:
    X = df
    y = None

# Identifier colonnes √† standardiser (pas les binaires 0/1)
cols_to_scale = []
for col in X.select_dtypes(include=[np.number]).columns:
    unique_vals = set(X[col].dropna().unique())
    if not unique_vals.issubset({0, 1, 0., 1.}):
        cols_to_scale.append(col)

print(f"   √Ä standardiser: {len(cols_to_scale)} colonnes")

# Standardisation
scaler = StandardScaler()
X[cols_to_scale] = scaler.fit_transform(X[cols_to_scale])
print("   StandardScaler appliqu√©")

# R√©assembler
if y is not None:
    df = pd.concat([X, y], axis=1)
else:
    df = X

print(f"‚úÖ √âtape 9 termin√©e - Dimensions: {df.shape}")

#===========================================
# SAUVEGARDE DANS PROCESSED/ (NOUVELLE √âTAPE)
print("\n" + "="*50)
print("AJOUT: SAUVEGARDE DATASET NETTOY√â")
print("="*50)

# PROCESSED_DIR d√©j√† d√©fini au d√©but du fichier
output_path = os.path.join(PROCESSED_DIR, "eda_results.csv")
df_clean.to_csv(output_path, index=False)

print(f"   üìÅ Dossier: {PROCESSED_DIR}")
print(f"   üíæ Dataset sauvegard√©: {output_path}")
print(f"   Dimensions: {df_clean.shape}")
print(f"   ‚ö†Ô∏è  Le dataset original dans raw/ est INTACT")

print("\n" + "="*50)
print("FIN DE L'EDA")
print("="*50)
